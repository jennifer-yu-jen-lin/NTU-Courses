---
title: "Homework 7"
author: "Yu-Jen Lin"
output:
  html_document: default
  pdf_document: default
---
#### b04b01036@ntu.edu.tw
#### --------------------------------------------------------------------------------------------------------------------------------------------------------

1. Use the “dominant” copepod species data (from HW1). Perform cluster analysis of stations based on percent composition data of the dominant species and tell your story about these copepod data.You can try to run the example analysis using the environmental data from HW3 to familiarize yourself with cluster analysis.

– Try to determine the effects of different distance measures
– Try to compare the results between different cluster algorithms
– Determine final number of clusters and describe differences among them

#### Retrieve the dominant species from HW1
```{r}
dominant = read.csv("~/R/dominant_HW1.csv",row.names = 1,header=T) # examples HW1
```

#### Installing packages
```{r}
# install.packages("vegan")
# install.packages("cluster")
# install.packages("fpc")
# install.packages("TWIX") # not available (for R version 3.4.4)
# install.packages('ggfortify')

library(ggfortify)
library(vegan)
library(cluster) 
library(fpc)
library(ggfortify)
# library(TWIX) # not available (for R version 3.4.4)
```

## Distance
```{r}
#standardize data
data = t(dominant)
data.std = scale(data)   # each column -> standardize (z score)

#compute distance matrix
dist.euclidean = dist(data.std,method='euclidean')
dist.maximum = dist(data.std,method='maximum')      
dist.manhattan = dist(data.std,method='manhattan')
dist.canberra = dist(data.std,method='canberra')    
dist.binary = dist(data.std,method='binary')         
dist.minkowski = dist(data.std,method='minkowski')   
dist.bray_curtis  = dist.manhattan - dist.maximum 
```

#### --------------------------------------------------------------------------------------------------------------------------------------------------------

# Conduct a Nonhierarchical Clustering (NHC)
## Try to determine the effects of different distance measures
#### Distance method: euclidean
```{r}
dist.METHOD = dist.euclidean

# Determine number of clusters
## by maximizing the distance between cluster centers while minimizing the within-cluster variation
## Need to write a program for making scree plot and silhouette width

km.result <- c() # Total within groups sum of squares
for(i in 1:15){
  km <- kmeans(dist.METHOD, centers = i, nstart = 25) # nstart: random sets that should be chosen # minimizing km
  
  # minimize k means # total within groups sum of squares
  km.result[i] <- km$tot.withinss # = sum(kmeans$withinss)
}
sil.result <- c()
for(i in 2:15){ # silhouette of 1 cluster is NA, thus I start from 2 clusters
  km <- kmeans(dist.METHOD, centers = i, nstart = 25) # nstart: random sets that should be chosen # minimizing km
  
  # silhouette width # maximize average silhouette
  ss = silhouette(km$cluster, dist(dist.METHOD)) # clusters >= 2
  sil.result[i] = mean(ss[,3])
}

# RESULTS
km.result.euclidean <- km.result
sil.result.euclidean <- sil.result
```

#### Distance method: maximum
```{r}
dist.METHOD = dist.maximum

# Determine number of clusters
## by maximizing the distance between cluster centers while minimizing the within-cluster variation
## Need to write a program for making scree plot and silhouette width

km.result <- c() # Within cluster sum
for(i in 1:15){
  km <- kmeans(dist.METHOD, centers = i, nstart = 25) # nstart: random sets that should be chosen # minimizing km
  
  # minimize k means # total within groups sum of squares
  km.result[i] <- km$tot.withinss # = sum(kmeans$withinss)
}
sil.result <- c()
for(i in 2:15){ # silhouette of 1 cluster is NA, thus I start from 2 clusters
  km <- kmeans(dist.METHOD, centers = i, nstart = 25) # nstart: random sets that should be chosen # minimizing km
  
  # silhouette width # maximize average silhouette
  ss = silhouette(km$cluster, dist(dist.METHOD)) # clusters >= 2
  sil.result[i] = mean(ss[,3])
}

# RESULTS
km.result.maximum <- km.result
sil.result.maximum <- sil.result
```

#### Distance method: manhattan
```{r}
dist.METHOD = dist.manhattan

# Determine number of clusters
## by maximizing the distance between cluster centers while minimizing the within-cluster variation
## Need to write a program for making scree plot and silhouette width

km.result <- c() # Within cluster sum
for(i in 1:15){
  km <- kmeans(dist.METHOD, centers = i, nstart = 25) # nstart: random sets that should be chosen # minimizing km
  
  # minimize k means # total within groups sum of squares
  km.result[i] <- km$tot.withinss # = sum(kmeans$withinss)
}
sil.result <- c()
for(i in 2:15){ # silhouette of 1 cluster is NA, thus I start from 2 clusters
  km <- kmeans(dist.METHOD, centers = i, nstart = 25) # nstart: random sets that should be chosen # minimizing km
  
  # silhouette width # maximize average silhouette
  ss = silhouette(km$cluster, dist(dist.METHOD)) # clusters >= 2
  sil.result[i] = mean(ss[,3])
}

# RESULTS
km.result.manhattan <- km.result
sil.result.manhattan <- sil.result
```

#### Distance method: canberra
```{r}
dist.METHOD = dist.canberra

# Determine number of clusters
## by maximizing the distance between cluster centers while minimizing the within-cluster variation
## Need to write a program for making scree plot and silhouette width

km.result <- c() # Within cluster sum
for(i in 1:15){
  km <- kmeans(dist.METHOD, centers = i, nstart = 25) # nstart: random sets that should be chosen # minimizing km
  
  # minimize k means # total within groups sum of squares
  km.result[i] <- km$tot.withinss # = sum(kmeans$withinss)
}
sil.result <- c()
for(i in 2:15){ # silhouette of 1 cluster is NA, thus I start from 2 clusters
  km <- kmeans(dist.METHOD, centers = i, nstart = 25) # nstart: random sets that should be chosen # minimizing km
  
  # silhouette width # maximize average silhouette
  ss = silhouette(km$cluster, dist(dist.METHOD)) # clusters >= 2
  sil.result[i] = mean(ss[,3])
}

# RESULTS
km.result.canberra <- km.result
sil.result.canberra <- sil.result
```

#### Distance method: binary
dist.METHOD = dist.binary

# Determine number of clusters
## by maximizing the distance between cluster centers while minimizing the within-cluster variation
## Need to write a program for making scree plot and silhouette width

km.result <- c() # Within cluster sum
for(i in 1:5){
  km <- kmeans(dist.METHOD, centers = i, nstart = 25) # nstart: random sets that should be chosen # minimizing km
  
  # minimize k means # total within groups sum of squares
  km.result[i] <- km$tot.withinss # = sum(kmeans$withinss)
}
sil.result <- c()
for(i in 2:15){ # silhouette of 1 cluster is NA, thus I start from 2 clusters
  km <- kmeans(dist.METHOD, centers = i, nstart = 25) # nstart: random sets that should be chosen # minimizing km
  
  # silhouette width # maximize average silhouette
  ss = silhouette(km$cluster, dist(dist.METHOD)) # clusters >= 2
  sil.result[i] = mean(ss[,3])
}

# RESULTS
km.result.binary <- km.result
sil.result.binary <- sil.result


#### Distance method: minkowski
```{r}
dist.METHOD = dist.minkowski

# Determine number of clusters
## by maximizing the distance between cluster centers while minimizing the within-cluster variation
## Need to write a program for making scree plot and silhouette width

km.result <- c() # Within cluster sum
for(i in 1:15){
  km <- kmeans(dist.METHOD, centers = i, nstart = 25) # nstart: random sets that should be chosen # minimizing km
  
  # minimize k means # total within groups sum of squares
  km.result[i] <- km$tot.withinss # = sum(kmeans$withinss)
}
sil.result <- c()
for(i in 2:15){ # silhouette of 1 cluster is NA, thus I start from 2 clusters
  km <- kmeans(dist.METHOD, centers = i, nstart = 25) # nstart: random sets that should be chosen # minimizing km
  
  # silhouette width # maximize average silhouette
  ss = silhouette(km$cluster, dist(dist.METHOD)) # clusters >= 2
  sil.result[i] = mean(ss[,3])
}

# RESULTS
km.result.minkowski <- km.result
sil.result.minkowski <- sil.result
```

#### Distance method: bray_curtis
```{r}
dist.METHOD = dist.bray_curtis

# Determine number of clusters
## by maximizing the distance between cluster centers while minimizing the within-cluster variation
## Need to write a program for making scree plot and silhouette width

km.result <- c() # Within cluster sum
for(i in 1:15){
  km <- kmeans(dist.METHOD, centers = i, nstart = 25) # nstart: random sets that should be chosen # minimizing km
  
  # minimize k means # total within groups sum of squares
  km.result[i] <- km$tot.withinss # = sum(kmeans$withinss)
}
sil.result <- c()
for(i in 2:15){ # silhouette of 1 cluster is NA, thus I start from 2 clusters
  km <- kmeans(dist.METHOD, centers = i, nstart = 25) # nstart: random sets that should be chosen # minimizing km
  
  # silhouette width # maximize average silhouette
  ss = silhouette(km$cluster, dist(dist.METHOD)) # clusters >= 2
  sil.result[i] = mean(ss[,3])
}

# RESULTS
km.result.bray_curtis <- km.result
sil.result.bray_curtis <- sil.result
```

## Determine how many clusters 
```{r}

#par(mfrow=c(1,2))
# k=3這個點上，曲線的變化率比較大


plot(km.result.euclidean, main="Distance method: euclidean", type='b', col="black", ylab="Total within groups sum of squares", xlab="Number of clusters k")
par(new = T)
plot(1:15, sil.result.euclidean, type='b', pch=19, frame=FALSE, ylab='', xlab='', axes=F, col="blue")
axis(side = 4, col="blue")
mtext(side = 4, line = 2, text="Average silhouette widths",col="blue")
abline(v=which.max(sil.result.euclidean), lty=2)
legend("topright", c("km","sil"), col=c("black","blue"), lty=1)

plot(km.result.maximum, main="Distance method: maximum", type='b', col="black", ylab="Total within groups sum of squares", xlab="Number of clusters k")
par(new = T)
plot(1:15, sil.result.maximum, type='b', pch=19, frame=FALSE, ylab='', xlab='', axes=F, col="blue")
axis(side = 4, col="blue")
mtext(side = 4, line = 2, text="Average silhouette widths",col="blue")
abline(v=which.max(sil.result.maximum), lty=2)
legend("topright", c("km","sil"), col=c("black","blue"), lty=1)

plot(km.result.manhattan, main="Distance method: manhattan", type='b', col="black", ylab="Total within groups sum of squares", xlab="Number of clusters k")
par(new = T)
plot(1:15, sil.result.manhattan, type='b', pch=19, frame=FALSE, ylab='', xlab='', axes=F, col="blue")
axis(side = 4, col="blue")
mtext(side = 4, line = 2, text="Average silhouette widths",col="blue")
abline(v=which.max(sil.result.manhattan), lty=2)
legend("topright", c("km","sil"), col=c("black","blue"), lty=1)

plot(km.result.canberra, main="Distance method: canberra", type='b', col="black", ylab="Total within groups sum of squares", xlab="Number of clusters k")
par(new = T)
plot(1:15, sil.result.canberra, type='b', pch=19, frame=FALSE, ylab='', xlab='', axes=F, col="blue")
axis(side = 4, col="blue")
mtext(side = 4, line = 2, text="Average silhouette widths",col="blue")
abline(v=which.max(sil.result.canberra), lty=2)
legend("topright", c("km","sil"), col=c("black","blue"), lty=1)

#plot(km.result.binary, main="Distance method: binary", type='b', col="black", ylab="Total within groups sum of squares", xlab="Number of clusters k")
#par(new = T)
#plot(1:15, sil.result.binary, type='b', pch=19, frame=FALSE, ylab='', xlab='', axes=F, col="blue")
#axis(side = 4, col="blue")
#mtext(side = 4, line = 2, text="Average silhouette widths",col="blue")
#abline(v=which.max(sil.result.binary), lty=2)
#legend("topright", c("km","sil"), col=c("black","blue"), lty=1)

plot(km.result.minkowski, main="Distance method: minkowski", type='b', col="black", ylab="Total within groups sum of squares", xlab="Number of clusters k")
par(new = T)
plot(1:15, sil.result.minkowski, type='b', pch=19, frame=FALSE, ylab='', xlab='', axes=F, col="blue")
axis(side = 4, col="blue")
mtext(side = 4, line = 2, text="Average silhouette widths",col="blue")
abline(v=which.max(sil.result.minkowski), lty=2)
legend("topright", c("km","sil"), col=c("black","blue"), lty=1)

plot(km.result.bray_curtis, main="Distance method: bray_curtis", type='b', col="black", ylab="Total within groups sum of squares", xlab="Number of clusters k")
par(new = T)
plot(1:15, sil.result.bray_curtis, type='b', pch=19, frame=FALSE, ylab='', xlab='', axes=F, col="blue")
axis(side = 4, col="blue")
mtext(side = 4, line = 2, text="Average silhouette widths",col="blue")
abline(v=which.max(sil.result.bray_curtis), lty=2)
legend("topright", c("km","sil"), col=c("black","blue"), lty=1)

```







```{r}
#compute k-means NHC around 8 mediods (clusters)

##  Partitioning Around Medoids / Partitioning (clustering) of the data into k clusters “around medoids”, a more robust version of K-means.
dist.METHOD <- dist.euclidean
y.pam = pam(dist.METHOD,k=8) 
plot(y.pam)
summary(y.pam) # 內容Objective function:,Isolated clusters:




## Clustering Large Applications
y.clara = clara(data.std,k=8, metric="euclidean") 
# plot(y.clara) ## ??? Error in princomp.default(x, scores = TRUE, cor = ncol(x) > 2) : 'princomp' can only be used with more units than variables

autoplot(prcomp(data.std), data = data.std, colour=y.pam$clustering, frame = TRUE, frame.colour = y.pam$clustering)
autoplot(prcomp(data.std), data = data.std, colour=y.pam$clustering,label=T, shape=F) # work



#evaluate cluster stability
y.eucl.boot = clusterboot(data.std, B=100, metric='euclidean', bootmethod=c('boot','subset'), clustermethod=claraCBI, usepam=TRUE, k=8, count=FALSE)
print(y.eucl.boot) # dissolved < 0.5, recovered > 0.75 
```




#### Test how the distance method calculate
```{r}
# dist.euclidean
sum.euclidean <- 0
for (i in 1:11){
sum.euclidean <- sum.euclidean + (data.std[1,i]-data.std[2,i])^2
}
sum.euclidean <- sum.euclidean ^ 0.5

# dist.manhattan
sum.manhattan <- 0
for (i in 1:11){
sum.manhattan <- sum.manhattan + abs(data.std[1,i]-data.std[2,i])
}
```

#### --------------------------------------------------------------------------------------------------------------------------------------------------------

## Conduct a Hierarchical Clustering (HC)
```{r}
dist.METHOD = dist.euclidean

# bottom-up (two ways of doing HC)
y.eucl.ward = hclust(dist.METHOD,method='ward.D') # Hierarchical Clustering
y.eucl.ward = agnes(dist.METHOD,method='ward') # Agglomerative Nesting (Hierarchical Clustering)
# top-down
y.eucl.dia = diana(dist.METHOD) # DIvisive ANAlysis Clustering


# Various hierarchical joining algorithms
y.eucl.ward.D = hclust(dist.METHOD,method='ward.D') 
y.eucl.ward.D2 = hclust(dist.METHOD,method='ward.D2') 
y.eucl.single = hclust(dist.METHOD,method='single') 
y.eucl.complete = hclust(dist.METHOD,method='complete') 
y.eucl.average = hclust(dist.METHOD,method='average') # = UPGMA 
y.eucl.mcquitty = hclust(dist.METHOD,method='mcquitty') # = WPGMA
y.eucl.median = hclust(dist.METHOD,method='median') # = WPGMC
y.eucl.centroid = hclust(dist.METHOD,method='centroid') # = UPGMC


y.eucl.ward = agnes(dist.METHOD,method='ward')
#  "average" ([unweighted pair-]group [arithMetic] average method, aka ‘UPGMA’), 
# "single" (single linkage), 
# "complete" (complete linkage), 
#"ward" (Ward's method),
#"weighted" (weighted average linkage, aka ‘WPGMA’), its generalization
# "flexible" which uses (a constant version of) the Lance-Williams formula and the par.method argument, and
# "gaverage" a generalized "average" aka “flexible UPGMA” method also using the Lance-Williams formula and par.method.
y.eucl.dia = diana(dist.METHOD)




# dendrogram

names <- data[,0]

names<-c("p1","p3","p4","p6","p13","p16","p19","p21","p23","p25","s18","s19","s20","s22","s23","s25","s27","s29","sA","sB","sC","sD","sE","sF","sG","w22","w23","w25","w27","w29","wA","wB","wC","wD")

data.2 <- cbind(data,names)

plot(y.eucl.ward.D,main='Wards-linkage Dendrogram',xlab='Station',labels=data.2[,44])
rect.hclust(y.eucl.ward.D,k=8) #cut by #clusters



plot(y.eucl.ward.D,main='Wards-linkage Dendrogram',xlab='Station',labels=data[,1])
rect.hclust(y.eucl.ward.D,h=8) #cut by height ?


#######


#aggolmerative coefficient
summary(y.eucl.ave)$ac 

#cophenetic correlation
cor(dist.eucl,cophenetic(y.eucl.ave))
plot(dist.eucl,cophenetic(y.eucl.ave),xlab="Original distances",ylab="Cophenetic distances")


1. Compare mean and SD of
the variables among clusters
2. Make boxplots
3. Univariate ANOVA or Kruskal-Wallis rank sum test to compare differences between cluster means for each variable





# aggolmerative coefficient
summary(y.eucl.average)  # ave ?

# cophenetic correlation
cor(dist.euclidean,cophenetic(y.eucl.average))
plot(dist.euclidean,cophenetic(y.eucl.average),xlab="Original distances",ylab="Cophenetic distances")

```


比 silhouette  Non hi or hi


